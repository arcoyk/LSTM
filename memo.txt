RNN

1. 文章をインプットとして文章の最後の単語の次に来る単語を予測するタスクを仮定する
2. 文章にはユニークな１００単語が含まれているとする
3. ウィンドウを４単語として最初の４単語をとってくる
4. １００次元のベクトルを０で初期化して４つの単語のIDの要素を１に初期化する
4. 初期化されたメモリ（１００次元のベクトル）とつなげて２００次元のベクトルにする
5. ニューラルネットに２００次元のベクトルを入力する
6. １００次元のベクトルが予測される（全ての要素はarctanで1から-1にマップされる）
7. このベクトルを次の学習のメモリとする
8. （正解値と間違っている分だけ重みを変更する）

これは１つまえのウィンドウまで覚えている。ウィンドウは４単語で設定したので５つ目はうまく行く（A cute dog runs [in])が１０つ目の予測は１つ目から３つ目の単語の予測を覚えていないのでうまくいかないかもしれない（A cute dog runs in a dirty wide nice [kitchen]。dogはkitchenにいないが A cute dogを忘れているため dirty wide niceに引っ張られてkitchenを予測した）。 

LSTM

1. RNNは通常通り１００次元のベクトルを予測する
2. IgnoreゲートはRNNの予測を今回の入力と前回の予測をもとにゲートする（RNNの予測のうちどれに注もすべきかを決める？）
3. Forgetゲートは前回の予測（１００次元ベクトル）を保持しており、これを今回の入力と前回の予測によりゲートして予測に足しこむ
4. Selectionゲートは今回の入力と前回の予測をもとにゲートする（構造はIgnoreと同じ。そのまま出すとメモリーが過剰に更新される？）

Squwasing function: 通常-1から1、0から1へのマップ関数。
Gate: 重み付けの別称。[1.0, 0.5, 0] gates [1, 1, 0] to produce [1.0, 0.5, 0]


