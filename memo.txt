手順
LSTMとプロセッシングを繋げる（インプットとアウトプットをプロセッシングで処理できる）
LSTMCellを導入 https://medium.com/@andre.holzner/lstm-cells-in-pytorch-fab924a78b1c
複数次元データの扱いを調べる（上野さんに聞く？）


RNN

1. 文章をインプットとして文章の最後の単語の次に来る単語を予測するタスクを仮定する
2. 文章にはユニークな１００単語が含まれているとする
3. ウィンドウを４単語として最初の４単語をとってくる
4. １００次元のベクトルを０で初期化して４つの単語のIDの要素を１に初期化する
4. 初期化されたメモリ（１００次元のベクトル）とつなげて２００次元のベクトルにする
5. ニューラルネットに２００次元のベクトルを入力する
6. １００次元のベクトルが予測される（全ての要素はarctanで1から-1にマップされる）
7. このベクトルを次の学習のメモリとする
8. （正解値と間違っている分だけ重みを変更する）

これは１つまえのウィンドウまで覚えている。ウィンドウは４単語で設定したので５つ目はうまく行く（A cute dog runs [in])が１０つ目の予測は１つ目から３つ目の単語の予測を覚えていないのでうまくいかないかもしれない（A cute dog runs in a dirty wide nice [kitchen]。dogはkitchenにいないが A cute dogを忘れているため dirty wide niceに引っ張られてkitchenを予測した）。 

LSTM

1. RNNは通常通り１００次元のベクトルを予測する
2. IgnoreゲートはRNNの予測を今回の入力と前回の予測をもとにゲートする（RNNの予測のうちどれに注もすべきかを決める？）
3. Forgetゲートは前回までの予測（１００次元ベクトル）を任意の時間保持しており、これを今回の入力と前回の予測によりゲートして予測に足しこむ。これが長期記憶を担う。
4. Selectionゲートは今回の入力と前回の予測をもとにゲートする（構造はIgnoreと同じ。そのまま出すとメモリーが過剰に更新される？）

Squwasing function: 通常-1から1、0から1へのマップ関数。
Gate: 重み付けの別称。[1.0, 0.5, 0] gates [1, 1, 0] to produce [1.0, 0.5, 0]
LSTM Cell: 
LSTM Layer: 
LSTM Block: 
num_units: Tensorflow実装におけるLSTMのforgetゲートに長期記憶を保持するための隠れ層の数。
backpropagation: loss関数の勾配をそれぞれの重みについて計算すること。

Pytorch
bach: データ列全体
mini-bach: データ列を複数集めたもの（複数のサンプル）
tensor: GPU処理が可能なn次元のベクトル
torch.FloatTensor: テンソルの型。torch.cuda.FloatTensorでGPU対応。
Variable: テンソル、勾配、勾配関数を保持するインスタンス。variable.dataはテンソル（torch.FloatTensor）で、variable.grad.dataで勾配。計算グラフを動的に生成する機能があり、loss = x.mm(w1).clamp().mm(w2).get_loss(y)として出てきたloss(Variable)はx, w1, w2（これもすべてVariable）をたどれ、バックプロパゲーションができる（loss.backward())。Tensorflowは計算グラフを作る処理と計算（重み付け、フォーワード）をする処理が分かれており、計算の最適化・分散処理が可能。逆に言えばPyTorchはunrolling（フォーワード中にネットを編集する）プログラミング上の柔軟性があるが、分散処理ができないため高速化ではTensorFlowに勝てない。計算グラフを作るところは似ているので、知識はそのまま使えそう。
from_numpy: numpy.ndarrayからテンソルを作る関数。
MSELoss: mean square error
nn.Modules: ニューラルネットの層に基本的な関数（フォーワードなど）を加えたクラス。__call__オペレターをオバーライドしておりmodule_name(variables)でフォーワードできる。アウトプットはVariable。model_name.parameters()で重み付けのVariableが取り出せる。
nn.Sequential: nn.Sequential(nn.Linear(input_n, hidden_n), nn.ReLU(), nn.Linear(hidden_n, out_n))でnn.modulesを繋げる。





